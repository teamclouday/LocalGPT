CHAT_UI_PORT=9000

WEBUI_NAME=LocalGPT

LLAMA_ARG_HOST=0.0.0.0
LLAMA_ARG_PORT=9090
LLAMA_ARG_THREADS=-1
LLAMA_ARG_CTX_SIZE=40960
LLAMA_ARG_N_PREDICT=-1
LLAMA_ARG_N_GPU_LAYERS=36 # 75% of layers
LLAMA_ARG_N_PARALLEL=4
LLAMA_ARG_NO_MMAP=1
LLAMA_ARG_NO_WEBUI=1
LLAMA_ARG_FLASH_ATTN=1
LLAMA_ARG_NO_CONTEXT_SHIFT=1


# Can override the following in a .env.local file


HF_TOKEN=
OPENAI_API_KEY=

# See https://github.com/ggml-org/llama.cpp/tree/master/examples/server
# python .\download.py -i "unsloth/Qwen3-30B-A3B-GGUF" -f "Qwen3-30B-A3B-Q4_K_M.gguf"
LLAMA_ARG_MODEL=/models/unsloth_Qwen3-30B-A3B-GGUF/Qwen3-30B-A3B-Q4_K_M.gguf