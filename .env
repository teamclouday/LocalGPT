CHAT_UI_PORT=9000
CHAT_UI_MONGO_PORT=27017

PUBLIC_APP_NAME=LocalGPT
PUBLIC_APP_ASSETS=chatui
PUBLIC_APP_COLOR=blue
PUBLIC_APP_DESCRIPTION="Local LLM inference"
PUBLIC_APP_DATA_SHARING=
PUBLIC_APP_DISCLAIMER=

LLAMA_ARG_HOST=0.0.0.0
LLAMA_ARG_PORT=9001
LLAMA_ARG_THREADS=-1
LLAMA_ARG_CTX_SIZE=4096
LLAMA_ARG_N_GPU_LAYERS=999
LLAMA_ARG_NO_MMAP=1

TEXT_EMBEDDINGS_PORT=9002
TEXT_EMBEDDINGS_MAX_BATCH_TOKENS=4000


# Can override the following in a .env.local file


HF_TOKEN=

# python .\download.py -i "Snowflake/snowflake-arctic-embed-m-v2.0" --exclude "*.onnx" "*.safetensors"
# python .\download.py -i "Snowflake/snowflake-arctic-embed-m-v2.0" -f "onnx/model.onnx"
TEXT_EMBEDDINGS_MODEL=/models/Snowflake_snowflake-arctic-embed-m-v2.0

# See https://github.com/ggml-org/llama.cpp/tree/master/examples/server
# python .\download.py -i "unsloth/Qwen3-30B-A3B-GGUF" -f "Qwen3-30B-A3B-Q4_0.gguf"
LLAMA_ARG_MODEL=/models/unsloth_Qwen3-30B-A3B-GGUF/Qwen3-30B-A3B-Q4_0.gguf

# See https://github.com/huggingface/chat-ui?tab=readme-ov-file#custom-models
MODELS="[
  {
    \"name\": \"unsloth/Qwen3-30B-A3B-GGUF\",
    \"displayName\": \"unsloth/Qwen3-30B-A3B-GGUF\",
    \"id\": \"unsloth/Qwen3-30B-A3B-GGUF\",
    \"endpoints\": [
      {
        \"type\": \"openai\",
        \"baseURL\": \"http://llama-cpp:9001/v1\",
      }
    ],
    \"reasoning\": {
      \"type\": \"tokens\",
      \"beginToken\": \"<think>\",
      \"endToken\": \"</think>\",
    }
  }
]"

# See https://github.com/huggingface/chat-ui?tab=readme-ov-file#text-embedding-models
TEXT_EMBEDDING_MODELS="[
  {
    \"name\": \"Snowflake/snowflake-arctic-embed-m-v2.0\",
    \"displayName\": \"Snowflake/snowflake-arctic-embed-m-v2.0\",
    \"description\": \"hosted embedding model\",
    \"chunkCharLength\": 768,
    \"preQuery\": \"query: \",
    \"prePassage\": \"passage: \",
    \"endpoints\": [
      {
        \"type\": \"tei\",
        \"url\": \"http://text-embeddings-inference:80/\",
      }
    ]
  }
]"