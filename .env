CHAT_UI_PORT=9000

WEBUI_NAME=LocalGPT

LLAMA_ARG_HOST=0.0.0.0
LLAMA_ARG_PORT=9090
LLAMA_ARG_THREADS=-1
LLAMA_ARG_CTX_SIZE=40960
LLAMA_ARG_N_PREDICT=-1
LLAMA_ARG_N_GPU_LAYERS=99
LLAMA_ARG_N_PARALLEL=2
LLAMA_ARG_NO_MMAP=1
LLAMA_ARG_NO_WEBUI=1
LLAMA_ARG_FLASH_ATTN=1
LLAMA_ARG_NO_CONTEXT_SHIFT=1

# Can override the following in a .env.local file


HF_TOKEN=
OPENAI_API_KEY=

# See https://github.com/ggml-org/llama.cpp/tree/master/tools/server
# python .\download.py -i "lmstudio-community/Qwen3-14B-GGUF" -f "Qwen3-14B-Q4_K_M.gguf"
LLAMA_ARG_MODEL=/models/lmstudio-community_Qwen3-14B-GGUF/Qwen3-14B-Q4_K_M.gguf